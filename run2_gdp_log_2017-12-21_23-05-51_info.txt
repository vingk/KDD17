--------------------------------------------------
Step 1: Load data
--------------------------------------------------
CSV Type: <class 'pandas.core.frame.DataFrame'>, Shape: (5914, 96)
  Number of irrelevant advisories  : 553
  Number of orphaned advisories (O): 20
  Number of started advisories  (S): 533
  Total number of advisories  (O+S): 553
  Number of cancellations       (C): 441
  Number of extensions          (E): 137
  Number of reductions          (R): 124
  Number of amendments          (A): 295
  Total number of changes (C+E+R+A): 997
  Number of unchangeds          (U): 3811
  Total number of hours (C+E+R+A+U): 4808
--------------------------------------------------
Step 2: Group into train and test sets by year
--------------------------------------------------
Year 2010
  Number of {C, E, R, A, U} samples: 97, 39, 17, 87, 922
  Total number of samples:           1162
Year 2011
  Number of {C, E, R, A, U} samples: 92, 23, 30, 67, 853
  Total number of samples:           1065
Year 2012
  Number of {C, E, R, A, U} samples: 111, 27, 37, 46, 858
  Total number of samples:           1079
Year 2013
  Number of {C, E, R, A, U} samples: 103, 31, 30, 71, 825
  Total number of samples:           1060
Year 2014
  Number of {C, E, R, A, U} samples: 38, 17, 10, 24, 353
  Total number of samples:           442
Train and test sets for year 2011
  Number of {C, E, R, A, U} train samples: 97, 39, 17, 87, 922
  Total number of train samples:           1162
  Number of {C, E, R, A, U} test samples:  92, 23, 30, 67, 853
  Total number of test samples:            1065
Train and test sets for year 2011 (merged into 3 classes)
  Number of {+, -, 0} train samples: 39, 114, 1009
  Total number of train samples:     1162
  Number of {+, -, 0} test samples:  23, 122, 920
  Total number of test samples:      1065
Train and test sets for year 2012
  Number of {C, E, R, A, U} train samples: 189, 62, 47, 154, 1775
  Total number of train samples:           2227
  Number of {C, E, R, A, U} test samples:  111, 27, 37, 46, 858
  Total number of test samples:            1079
Train and test sets for year 2012 (merged into 3 classes)
  Number of {+, -, 0} train samples: 62, 236, 1929
  Total number of train samples:     2227
  Number of {+, -, 0} test samples:  27, 148, 904
  Total number of test samples:      1079
Train and test sets for year 2013
  Number of {C, E, R, A, U} train samples: 300, 89, 84, 200, 2633
  Total number of train samples:           3306
  Number of {C, E, R, A, U} test samples:  103, 31, 30, 71, 825
  Total number of test samples:            1060
Train and test sets for year 2013 (merged into 3 classes)
  Number of {+, -, 0} train samples: 89, 384, 2833
  Total number of train samples:     3306
  Number of {+, -, 0} test samples:  31, 133, 896
  Total number of test samples:      1060
Train and test sets for year 2014
  Number of {C, E, R, A, U} train samples: 403, 120, 114, 271, 3458
  Total number of train samples:           4366
  Number of {C, E, R, A, U} test samples:  38, 17, 10, 24, 353
  Total number of test samples:            442
Train and test sets for year 2014 (merged into 3 classes)
  Number of {+, -, 0} train samples: 120, 517, 3729
  Total number of train samples:     4366
  Number of {+, -, 0} test samples:  17, 48, 377
  Total number of test samples:      442
--------------------------------------------------
Step 3: Calculate train and test sets indices
--------------------------------------------------
Number of samples in Xtrainall: 4808
Number of samples in ytrainall: 4808
  Number of samples in trainindiceslist[0]: 1162
  Number of samples in testindiceslist [0]: 1065
  Number of samples in trainindiceslist[1]: 2227
  Number of samples in testindiceslist [1]: 1079
  Number of samples in trainindiceslist[2]: 3306
  Number of samples in testindiceslist [2]: 1060
  Number of samples in trainindiceslist[3]: 4366
  Number of samples in testindiceslist [3]: 442
--------------------------------------------------
Step 4: Run Naive Bayes
--------------------------------------------------
Running Naive Bayes on train and test sets for year 2011 and classes ('+', '-', '0')
  Accuracy        : 0.795305164319
  Macro recall    : 0.451003801378
  Macro precision : 0.45707387828
  Macro F1 score  : 0.431202650297
  Confusion matrix:
[[  5   4  14]
 [ 19  31  72]
 [ 69  40 811]]
Running Naive Bayes on train and test sets for year 2012 and classes ('+', '-', '0')
  Accuracy        : 0.796107506951
  Macro recall    : 0.461383581221
  Macro precision : 0.49122102022
  Macro F1 score  : 0.455419111598
  Confusion matrix:
[[  5   5  17]
 [ 23  45  80]
 [ 59  36 809]]
Running Naive Bayes on train and test sets for year 2013 and classes ('+', '-', '0')
  Accuracy        : 0.77358490566
  Macro recall    : 0.477742996604
  Macro precision : 0.458852287853
  Macro F1 score  : 0.452330239038
  Confusion matrix:
[[  8   5  18]
 [ 18  42  73]
 [ 67  59 770]]
Running Naive Bayes on train and test sets for year 2014 and classes ('+', '-', '0')
  Accuracy        : 0.776018099548
  Macro recall    : 0.531844324821
  Macro precision : 0.474656084656
  Macro F1 score  : 0.471511359595
  Confusion matrix:
[[  8   2   7]
 [ 14  13  21]
 [ 34  21 322]]
--------------------------------------------------
Step 5: Find Best Parameters for Decision Tree Classifier using Grid Search
--------------------------------------------------
Running Grid Search on Decision Tree Classifier on classes ('+', '-', '0')
  Best parameters         : {'min_impurity_decrease': 0.01, 'max_leaf_nodes': 10, 'min_samples_leaf': 0.02, 'min_samples_split': 0.08, 'min_weight_fraction_leaf': 0.0, 'random_state': 581298211, 'max_features': 3, 'max_depth': 3, 'class_weight': 'balanced'}
  Best score              : 0.484183226949
  Best feature importances: [ 0.          0.          0.          0.24050716  0.50059025  0.25890259]
--------------------------------------------------
Step 6: Run Decision Tree Classifier using Best Parameters
--------------------------------------------------
Running Decision Tree Classifier on train and test sets for year 2011 and classes ('+', '-', '0')
  Accuracy        : 0.750234741784
  Macro recall    : 0.648651698741
  Macro precision : 0.479319621623
  Macro F1 score  : 0.500190158358
  Confusion matrix:
[[ 12   6   5]
 [ 22  80  20]
 [ 82 131 707]]
Running Decision Tree Classifier (with SMOTE) on train and test sets for year 2011 and classes ('+', '-', '0')
  Accuracy        : 0.731455399061
  Macro recall    : 0.601205749584
  Macro precision : 0.460952024456
  Macro F1 score  : 0.469373614191
  Confusion matrix:
[[ 11   9   3]
 [ 35  69  18]
 [ 91 130 699]]
Running Decision Tree Classifier (with TOMEK) on train and test sets for year 2011 and classes ('+', '-', '0')
  Accuracy        : 0.752112676056
  Macro recall    : 0.637615823236
  Macro precision : 0.4774852972
  Macro F1 score  : 0.498344113127
  Confusion matrix:
[[ 11   7   5]
 [ 21  81  20]
 [ 78 133 709]]
Running Decision Tree Classifier on train and test sets for year 2012 and classes ('+', '-', '0')
  Accuracy        : 0.746061167748
  Macro recall    : 0.577905338967
  Macro precision : 0.473701108449
  Macro F1 score  : 0.491960957721
  Confusion matrix:
[[  9  12   6]
 [ 23  92  33]
 [ 57 143 704]]
Running Decision Tree Classifier (with SMOTE) on train and test sets for year 2012 and classes ('+', '-', '0')
  Accuracy        : 0.727525486562
  Macro recall    : 0.583327427723
  Macro precision : 0.474723814853
  Macro F1 score  : 0.473594567939
  Confusion matrix:
[[ 14   7   6]
 [ 48  67  33]
 [103  97 704]]
Running Decision Tree Classifier (with TOMEK) on train and test sets for year 2012 and classes ('+', '-', '0')
  Accuracy        : 0.724745134384
  Macro recall    : 0.579805838287
  Macro precision : 0.477284189334
  Macro F1 score  : 0.461645603009
  Confusion matrix:
[[ 16   4   7]
 [ 60  53  35]
 [117  74 713]]
Running Decision Tree Classifier on train and test sets for year 2013 and classes ('+', '-', '0')
  Accuracy        : 0.721698113208
  Macro recall    : 0.583592928895
  Macro precision : 0.463092208019
  Macro F1 score  : 0.477217691725
  Confusion matrix:
[[ 14  14   3]
 [ 26  72  35]
 [ 85 132 679]]
Running Decision Tree Classifier (with SMOTE) on train and test sets for year 2013 and classes ('+', '-', '0')
  Accuracy        : 0.707547169811
  Macro recall    : 0.578984633964
  Macro precision : 0.459913909991
  Macro F1 score  : 0.457612440161
  Confusion matrix:
[[ 18  10   3]
 [ 45  53  35]
 [122  95 679]]
Running Decision Tree Classifier (with TOMEK) on train and test sets for year 2013 and classes ('+', '-', '0')
  Accuracy        : 0.719811320755
  Macro recall    : 0.586826820074
  Macro precision : 0.462109890686
  Macro F1 score  : 0.475696273264
  Confusion matrix:
[[ 15  13   3]
 [ 29  69  35]
 [ 86 131 679]]
Running Decision Tree Classifier on train and test sets for year 2014 and classes ('+', '-', '0')
  Accuracy        : 0.723981900452
  Macro recall    : 0.502733785822
  Macro precision : 0.436998806264
  Macro F1 score  : 0.443332353387
  Confusion matrix:
[[  3  11   3]
 [ 11  27  10]
 [ 31  56 290]]
Running Decision Tree Classifier (with SMOTE) on train and test sets for year 2014 and classes ('+', '-', '0')
  Accuracy        : 0.723981900452
  Macro recall    : 0.502733785822
  Macro precision : 0.436998806264
  Macro F1 score  : 0.443332353387
  Confusion matrix:
[[  3  11   3]
 [ 11  27  10]
 [ 31  56 290]]
Running Decision Tree Classifier (with TOMEK) on train and test sets for year 2014 and classes ('+', '-', '0')
  Accuracy        : 0.723981900452
  Macro recall    : 0.502733785822
  Macro precision : 0.436998806264
  Macro F1 score  : 0.443332353387
  Confusion matrix:
[[  3  11   3]
 [ 11  27  10]
 [ 31  56 290]]
--------------------------------------------------
Step 7: Calculate f-statistic
--------------------------------------------------
Calculating f-statistic for Decision Tree Classifier (No resampling vs SMOTE oversampling vs TOMEK undersampling)
  F-statistic: 0.655903683372, P-Value: 0.542106645304
--------------------------------------------------
Step 8: Calculate t-statistic
--------------------------------------------------
Calculating t-statistic for Naive Bayes and Decision Tree Classifier (No resampling)
  T-statistic: -1.69982849046, P-Value: 0.147680244277
--------------------------------------------------
Step 9: Fit Decision Tree to Whole Dataset and Export to File
--------------------------------------------------
Running Decision Tree Classifier on whole dataset
  Decision Tree Exported to File:gdp_tree_2017-12-21_23-05-51.png
